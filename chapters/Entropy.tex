Энтропия используется в различных областях науки (термодинамика, биология, математическая статистика и др.). В данной работе мы будем обращаться к теории информации, поэтому определим понятие информационной энтропии:

{\bfseries Энтропия (информационная)} --- мера неопределенности случайной величины или системы. В случае дискретных случайных величин она показывает минимальное среднее число бит для шифрования информации о значениях, которые эта случайная величина принимает \cite{entropy}.

Например, мы собираемся отправлять сообщения о результатах нашего эксперимента, где возможны 4 результата: {\itshape огонь, вода, земля, воздух}. Также мы знаем, с какой вероятностью наступает каждый из них:

\begin{center}
\setlength{\extrarowheight}{3mm}
\begin{tabular}{|c|c|c|c|c|}
	\hline
	Исход & Огонь & Вода & Земля & Воздух\\[2mm]
	\hline
	Вероятность исхода & $\displaystyle\frac{1}{4}$ & $\displaystyle\frac{1}{2}$ & $\displaystyle\frac{1}{8}$ & $\displaystyle\frac{1}{8}$\\[3mm]
	\hline
\end{tabular}
\end{center}

--- распределение $\tilde P$.

Чтобы сделать сообщение как более коротким, мы можем закодировать каждое слово набором символов из 0 и 1 (1 символ = 1 бит). При большей мощности алфавита объем памяти, нужный для одного символа, возрастает --- оставим двузначную систему. Также, заметим, что необходимо следить за тем, чтобы наше сообщение можно было однозначно расшифровать. Если мы закодируем «огонь» как 0, «воду» как 00, то сообщение 0000 не сможет быть декодировано.

Допустим, мы зашифровали слова следующим образом: «огонь» --- 10, «вода» --- 0, «земля» --- 110, «воздух» --- 111. Тогда ожидаемое число бит на один исход эксперимента:\\[2mm] $\displaystyle \frac{1}{4} \cdot 2 + \frac{1}{2} \cdot 1 + \frac{1}{8} \cdot 3 + \frac{1}{8} \cdot 3 = 1,75$ бит.
\\[2mm]
\indent Данный объем является оптимальным: более редкие слова кодируются большим числом символов, а более частые --- меньшим. Это позволяет использовать наименьшее число символов при отправке сообщения: хотя «земля» и «воздух» требуют 3 бита, они происходят реже, чем остальные, а значит, позволяют нам использовать меньшее число символов для событий, про которые мы будем писать чаще.

Найденный средний объем символов на один исход и есть энтропия:
\begin{equation}
	H(\tilde P) = \sum_{i=1}^{n}\tilde{p_i} \cdot \log_2 {\frac{1}{\tilde p_i}} =- \sum_{i=1}^{n}\tilde{p_i} \cdot \log_2 \tilde{p_i} \, ,
	\label{entropy}
\end{equation}
где $\tilde p_i$ --- вероятность того, что в результате эксперимента получится $i$-ый исход.

Формула \eqref{entropy} используется для дискретных случайных величин. Из нее видно, что количество бит на одно слово, выраженное в $\displaystyle\log_2 {\frac{1}{\tilde p_i}}$, обратно пропорционально вероятности, с которой встречается это слово.

При отсутствии неопределенности средний объем символов равен 0, поскольку нам не нужно отправлять сообщение --- информация общеизвестна. Именно это характеризует энтропию как меру неопределенности --- появление случайности требует плату за получение информации в размере энтропии.