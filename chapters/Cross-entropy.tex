Разобравшись с энтропией, мы можем перейти к кросс-энтропии --- именно ее мы будем использовать в данной работе.

{\bfseries Кросс-энтропия} --- количество информации, в среднем, необходимое для опознания событий из распределения $P$, используя оптимальную схему для распределения $\tilde P$ \cite{entropy}.

Возвращаясь к предыдущему примеру: допустим, что мы ошиблись с вероятностями результатов экспериментов, и, на самом деле, они выглядят вот так:

\begin{center}
\setlength{\extrarowheight}{3mm}
\begin{tabular}{|c|c|c|c|c|}
	\hline
	Исход & Огонь & Вода & Земля & Воздух\\[2mm]
	\hline
	Вероятность исхода & $\displaystyle\frac{1}{8}$ & $\displaystyle\frac{1}{4}$ & $\displaystyle\frac{1}{4}$ & $\displaystyle\frac{3}{8}$\\[3mm]
	\hline
\end{tabular}
\end{center}

--- распределение $P$.

Но мы не знаем этого и продолжаем отправлять сообщения, используя код для прежнего распределения $\tilde P$. Тогда настоящее ожидаемое среднее число бит на один исход эксперимента: $\displaystyle\frac{1}{8} \cdot 2  + \frac{1}{4} \cdot 1  + \frac{1}{4} \cdot 3 + \frac{3}{8} \cdot 3 = 2,375$ бит. Данная величина представляет собой кросс-энтропию:
\begin{equation}
	H_P(\tilde P) = - \sum_{i=1}^{n} p_i \cdot \log_2 \tilde{p_i}
\end{equation}

Получившийся объем не является оптимальным как для истинного распределения, так и для используемого. Поэтому кросс-энтропия больше (либо равна, если $p = \tilde p$) энтропии. Между тем, разница между кросс-энтропией показывает, насколько сильно отличается истинное распределение $P$ от используемого для кодирования $\tilde P$.

{\bfseries Дивергенция Кульбака-Лейблера} --- степень отдаленности одного вероятностного распределения $P$ от другого $\tilde P$ \cite{entropy}.

\begin{equation}
	D_{KL}(P\, ||\, \tilde P)= H_P(\tilde P) - H(P) = - \sum\limits_{i=1}^n p_i\log \tilde p_i + \sum\limits_{i=1}^n p_i\log p_i
\end{equation}

Минимизация дивергенции Кульбака-Лейблера по $\tilde p$, используемому обыкновенно вместо истинного, позволяет максимально приблизить выбранное $\tilde P$ к $P$. Это дает возможность взять распределение, очень близкое к истинному.